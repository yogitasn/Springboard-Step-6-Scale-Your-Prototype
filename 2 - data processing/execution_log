
(base) C:\Users\yogit>conda create --name dbconnect python=3.7.5
Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: C:\Users\yogit\anaconda3\envs\dbconnect

  added / updated specs:
    - python=3.7.5


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    certifi-2020.12.5          |   py37haa95532_0         141 KB
    pip-21.0.1                 |   py37haa95532_0         1.8 MB
    python-3.7.5               |       h8c8aaf0_0        14.7 MB
    setuptools-52.0.0          |   py37haa95532_0         711 KB
    wincertstore-0.2           |           py37_0          14 KB
    ------------------------------------------------------------
                                           Total:        17.4 MB

The following NEW packages will be INSTALLED:

  ca-certificates    pkgs/main/win-64::ca-certificates-2021.1.19-haa95532_0
  certifi            pkgs/main/win-64::certifi-2020.12.5-py37haa95532_0
  openssl            pkgs/main/win-64::openssl-1.1.1j-h2bbff1b_0
  pip                pkgs/main/win-64::pip-21.0.1-py37haa95532_0
  python             pkgs/main/win-64::python-3.7.5-h8c8aaf0_0
  setuptools         pkgs/main/win-64::setuptools-52.0.0-py37haa95532_0
  sqlite             pkgs/main/win-64::sqlite-3.33.0-h2a8f88b_0
  vc                 pkgs/main/win-64::vc-14.2-h21ff451_1
  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.27.29016-h5e58377_2
  wheel              pkgs/main/noarch::wheel-0.36.2-pyhd3eb1b0_0
  wincertstore       pkgs/main/win-64::wincertstore-0.2-py37_0
  zlib               pkgs/main/win-64::zlib-1.2.11-h62dcd97_4


Proceed ([y]/n)? y


Downloading and Extracting Packages
wincertstore-0.2     | 14 KB     | ############################################################################ | 100%
setuptools-52.0.0    | 711 KB    | ############################################################################ | 100%
certifi-2020.12.5    | 141 KB    | ############################################################################ | 100%
python-3.7.5         | 14.7 MB   | ############################################################################ | 100%
pip-21.0.1           | 1.8 MB    | ############################################################################ | 100%
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate dbconnect
#
# To deactivate an active environment, use
#
#     $ conda deactivate


(base) C:\Users\yogit>conda activate dbconnect

(dbconnect) C:\Users\yogit>pip install -U databricks-connect==7.1
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\Scripts\pip-script.py", line 6, in <module>
    from pip._internal.cli.main import main
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\cli\main.py", line 8, in <module>
    from pip._internal.cli.autocompletion import autocomplete
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\cli\autocompletion.py", line 9, in <module>
    from pip._internal.cli.main_parser import create_main_parser
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\cli\main_parser.py", line 7, in <module>
    from pip._internal.cli import cmdoptions
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\cli\cmdoptions.py", line 22, in <module>
    from pip._internal.cli.progress_bars import BAR_TYPES
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\cli\progress_bars.py", line 9, in <module>
    from pip._internal.utils.logging import get_indentation
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\utils\logging.py", line 14, in <module>
    from pip._internal.utils.misc import ensure_dir
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\utils\misc.py", line 29, in <module>
    from pip._internal.locations import get_major_minor_version, site_packages, user_site
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pip\_internal\locations.py", line 11, in <module>
    from distutils.command.install import SCHEME_KEYS
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\distutils\command\install.py", line 9, in <module>
    from distutils.core import Command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\distutils\core.py", line 17, in <module>
    from distutils.cmd import Command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\distutils\cmd.py", line 9, in <module>
    from distutils import util, dir_util, file_util, archive_util, dep_util
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt

(dbconnect) C:\Users\yogit>pip install -U databricks-connect==7.5
ERROR: Could not find a version that satisfies the requirement databricks-connect==7.5
ERROR: No matching distribution found for databricks-connect==7.5

(dbconnect) C:\Users\yogit>pip install -U databricks-connect==7.3
ERROR: Could not find a version that satisfies the requirement databricks-connect==7.3
ERROR: No matching distribution found for databricks-connect==7.3

(dbconnect) C:\Users\yogit>databricks-connect==7.3.* package
'databricks-connect' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Users\yogit>pip install databricks-connect==7.3.* package
Collecting databricks-connect==7.3.*
  Downloading databricks-connect-7.3.9.tar.gz (222.2 MB)
     |████████████████████████████████| 222.2 MB 51 kB/s
Collecting package
  Downloading package-0.1.1.tar.gz (13 kB)
    ERROR: Command errored out with exit status 1:
     command: 'C:\Users\yogit\anaconda3\envs\dbconnect\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\yogit\\AppData\\Local\\Temp\\pip-install-w9z0f_8y\\package_58f978c1586d4545aa84dbf789c19d02\\setup.py'"'"'; __file__='"'"'C:\\Users\\yogit\\AppData\\Local\\Temp\\pip-install-w9z0f_8y\\package_58f978c1586d4545aa84dbf789c19d02\\setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' egg_info --egg-base 'C:\Users\yogit\AppData\Local\Temp\pip-pip-egg-info-xxpkye5b'
         cwd: C:\Users\yogit\AppData\Local\Temp\pip-install-w9z0f_8y\package_58f978c1586d4545aa84dbf789c19d02\
    Complete output (6 lines):
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "C:\Users\yogit\AppData\Local\Temp\pip-install-w9z0f_8y\package_58f978c1586d4545aa84dbf789c19d02\setup.py", line 31
        """
          ^
    SyntaxError: invalid syntax
    ----------------------------------------
WARNING: Discarding https://files.pythonhosted.org/packages/27/16/89ea913b3e70256b9abe4f222543553fcce8bafc7ff3774a8802a054e6b8/package-0.1.1.tar.gz#sha256=01eee19a56a936bd63222f0a3c531fcdba37b5ad1bd833b960d62fb960b4955e (from https://pypi.org/simple/package/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
ERROR: Could not find a version that satisfies the requirement package
ERROR: No matching distribution found for package

(dbconnect) C:\Users\yogit>pip install databricks-connect==7.3.7
Collecting databricks-connect==7.3.7
  Downloading databricks-connect-7.3.7.tar.gz (222.1 MB)
     |████████████████████████████████| 222.1 MB 35 kB/s
Collecting py4j==0.10.9
  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)
Collecting six
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
WARNING: The candidate selected for download or install is a yanked version: 'databricks-connect' candidate (version 7.3.7 at https://files.pythonhosted.org/packages/cb/f7/aa6f3f51b4f3bd34265038095c2a539f24c53852b7edd6f1c52594299343/databricks-connect-7.3.7.tar.gz#sha256=35ead50a0550e65a7d6fd78e2c8e54095b53514fba85180768a2dbcdd3f2cf0b (from https://pypi.org/simple/databricks-connect/))
Reason for being yanked: Unsupported release. Please use the latest release for the Databricks Runtime Version.
Building wheels for collected packages: databricks-connect
  Building wheel for databricks-connect (setup.py) ... done
  Created wheel for databricks-connect: filename=databricks_connect-7.3.7-py2.py3-none-any.whl size=222690439 sha256=5aa7b5ad608a7998065fa28df7e5afb051e8b524cf91258c85d778f9671986a8
  Stored in directory: c:\users\yogit\appdata\local\pip\cache\wheels\09\77\de\3bcff976010178e3f351500b028e1f6039fb4f208b3df67172
Successfully built databricks-connect
Installing collected packages: six, py4j, databricks-connect
Successfully installed databricks-connect-7.3.7 py4j-0.10.9 six-1.15.0

(dbconnect) C:\Users\yogit>databricks-connect configure
The current configuration is:
* Databricks Host: https://adb-1387481425025837.17.azuredatabricks.net/
* Databricks Token: dapie4efd256a53a0f286c7ef804567a0742-3
* Cluster ID: 0306-163505-barb59
* Org ID: 1387481425025837
* Port: 15001
Set new config values (leave input empty to accept default):
Databricks Host [https://adb-1387481425025837.17.azuredatabricks.net/]: https://adb-1387481425025837.17.azuredatabricks.net/

*** IMPORTANT: For AAD token users, please leave this empty and set AAD token via spark conf, spark.databricks.service.token

Databricks Token [dapie4efd256a53a0f286c7ef804567a0742-3]: dapie4efd256a53a0f286c7ef804567a0742-3

*** IMPORTANT: please ensure that your cluster has:
- Databricks Runtime version of DBR 5.1+
- Python version same as your local Python (i.e., 2.7 or 3.5)
- the Spark conf `spark.databricks.service.server.enabled true` set

Cluster ID (e.g., 0921-001415-jelly628) [0306-163505-barb59]: 0306-174454-gears22
Org ID (Azure-only, see ?o=orgId in URL) [1387481425025837]: 1387481425025837
Port [15001]: 15001

Updated configuration in C:\Users\yogit/.databricks-connect
* Spark jar dir: c:\users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark/jars
* Spark home: c:\users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark
* Run `pip install -U databricks-connect` to install updates
* Run `pyspark` to launch a Python shell
* Run `spark-shell` to launch a Scala shell
* Run `databricks-connect test` to test connectivity

(dbconnect) C:\Users\yogit>databricks-connect test
* PySpark is installed at c:\users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark
* Checking SPARK_HOME
* Checking java version
java version "1.8.0_261"
Java(TM) SE Runtime Environment (build 1.8.0_261-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)
* Skipping scala command test on Windows
* Testing python command
21/03/06 12:53:12 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 12:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 12:53:15 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 12:53:28 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
* Simple PySpark test passed                                        (0 + 1) / 1]
* Testing dbutils.fs
[FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0), FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0), FileInfo(path='dbfs:/tmp/', name='tmp/', size=0)]
* Simple dbutils test passed
* All tests passed.

(dbconnect) C:\Users\yogit>SUCCESS: The process with PID 23120 (child process of PID 20776) has been terminated.
SUCCESS: The process with PID 20776 (child process of PID 15140) has been terminated.
SUCCESS: The process with PID 15140 (child process of PID 5652) has been terminated.

(dbconnect) C:\Users\yogit>pyspark
Python 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
21/03/06 12:54:36 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 12:54:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 12:54:40 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.1-SNAPSHOT
      /_/

Using Python version 3.7.5 (default, Oct 31 2019 15:18:51)
SparkSession available as 'spark'.
>>>
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 287, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> Terminate batch job (Y/N)? Y
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'Y' is not defined
>>>
>>> u
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'u' is not defined
>>> 1387481425025837
1387481425025837
>>> ^CERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:64750)
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1193, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1033, in send_command
    response = connection.send_command(command)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1197, in send_command
    "Error while sending", e, proto.ERROR_ON_SEND)
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1193, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1033, in send_command
    response = connection.send_command(command)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1197, in send_command
    "Error while sending", e, proto.ERROR_ON_SEND)
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1193, in send_command
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1033, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1197, in send_command
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1193, in send_command
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1033, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1197, in send_command
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 286, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 1183, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1303, in __call__
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1048, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1048, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1031, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 979, in _get_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 985, in _create_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1127, in start
py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:64750)
>>> ^X^Z
  File "<stdin>", line 1
    ↑→
    ^
SyntaxError: invalid syntax
>>> ^Z^CERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:64750)
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 286, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 1183, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1303, in __call__
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1031, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 979, in _get_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 985, in _create_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1127, in start
py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:64750)
>>> ^CERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:64750)
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 286, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 1183, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1303, in __call__
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1031, in send_command
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 979, in _get_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 985, in _create_connection
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1127, in start
py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:64750)
>>> ^Z^D

ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:64750)
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py:463: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.
  RuntimeWarning
SUCCESS: The process with PID 23540 (child process of PID 12012) has been terminated.
SUCCESS: The process with PID 12012 (child process of PID 5316) has been terminated.
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:64750)
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
^CTerminate batch job (Y/N)? Y
Terminate batch job (Y/N)? Y

(dbconnect) C:\Users\yogit>spark-shell
21/03/06 12:55:47 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 12:55:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 12:55:56 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Spark context Web UI available at http://DESKTOP-4281265:4040
Spark context available as 'sc' (master = local[*], app id = local-1615053357184).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1-SNAPSHOT
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_261)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.range(100).reduce(_ + _)
<console>:24: error: overloaded method value reduce with alternatives:
  (func: org.apache.spark.api.java.function.ReduceFunction[java.lang.Long])java.lang.Long <and>
  (func: (java.lang.Long, java.lang.Long) => java.lang.Long)java.lang.Long
 cannot be applied to ((java.lang.Long, java.lang.Long) => scala.Long)
       spark.range(100).reduce(_ + _)
                        ^

scala> spark.range
<console>:24: error: ambiguous reference to overloaded definition,
both method range in class SparkSession of type (start: Long, end: Long, step: Long, numPartitions: Int)org.apache.spark.sql.Dataset[Long]
and  method range in class SparkSession of type (start: Long, end: Long, step: Long)org.apache.spark.sql.Dataset[Long]
match expected type ?
       spark.range
             ^

scala> spark.range(21/03/06 12:56:11 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
100).reduce(_ + _)
<console>:24: error: overloaded method value reduce with alternatives:
  (func: org.apache.spark.api.java.function.ReduceFunction[java.lang.Long])java.lang.Long <and>
  (func: (java.lang.Long, java.lang.Long) => java.lang.Long)java.lang.Long
 cannot be applied to ((java.lang.Long, java.lang.Long) => scala.Long)
       spark.range(100).reduce(_ + _)
                        ^

scala> spark.range(100)
res3: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val rdd=spark.range(100)
rdd: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> rdd.show(3)
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+
only showing top 3 rows


scala> rdd.reduce(+_+)
<console>:26: error: missing parameter type for expanded function ((x$1: <error>) => x$1.unary_$plus.$plus)
       rdd.reduce(+_+)
                   ^

scala> rdd.reduce(_+_)
<console>:26: error: overloaded method value reduce with alternatives:
  (func: org.apache.spark.api.java.function.ReduceFunction[java.lang.Long])java.lang.Long <and>
  (func: (java.lang.Long, java.lang.Long) => java.lang.Long)java.lang.Long
 cannot be applied to ((java.lang.Long, java.lang.Long) => scala.Long)
       rdd.reduce(_+_)
           ^

scala> Terminate batch job (Y/N)?
Terminate batch job (Y/N)? Y
Terminate batch job (Y/N)? Y

(dbconnect) C:\Users\yogit>https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkU
'https:' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Users\yogit>Run databricks-connect get-jar-dir
'Run' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Users\yogit>databricks-connect get-jar-dir
c:\users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark/jars

(dbconnect) C:\Users\yogit>cd ..

(dbconnect) C:\Users>cd ..

(dbconnect) C:\>cd "Step 6 - Scale Your Prototype"

(dbconnect) C:\Step 6 - Scale Your Prototype>cd "2 - data processing"

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python test.py
21/03/06 13:12:26 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:12:30 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Testing simple count
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
21/03/06 13:12:48 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
100

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 11328 (child process of PID 13892) has been terminated.
SUCCESS: The process with PID 13892 (child process of PID 18860) has been terminated.
SUCCESS: The process with PID 18860 (child process of PID 16428) has been terminated.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>
(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python databricks.py
Traceback (most recent call last):
  File "databricks.py", line 6, in <module>
    if not any(mount.mountPoint == '/mnt/FileStore/MountFolder/' for mount in dbutils.fs.mounts()):
NameError: name 'dbutils' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python test.py
21/03/06 13:19:50 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:19:53 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Traceback (most recent call last):
  File "test.py", line 17, in <module>
    if not any(mount.mountPoint == '/mnt/FileStore/MountFolder/' for mount in dbutils.fs.mounts()):
NameError: name 'dbutils' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 9624 (child process of PID 23660) has been terminated.
SUCCESS: The process with PID 23660 (child process of PID 24108) has been terminated.
SUCCESS: The process with PID 24108 (child process of PID 1256) has been terminated.
python test.py
Traceback (most recent call last):
  File "test.py", line 3, in <module>
    spark = SparkSession\
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\session.py", line 189, in getOrCreate
^C    sc = SparkContext.getOrCreate(sparkConf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 333, in _ensure_initialized
Terminate batch job (Y/N)?     SparkContext._gateway = gateway or launch_gateway(conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\java_gateway.py", line 102, in launch_gateway
    time.sleep(0.1)
KeyboardInterrupt

Terminate batch job (Y/N)?

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python test.py
21/03/06 13:21:32 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:21:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:21:36 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Traceback (most recent call last):
  File "test.py", line 17, in <module>
    if not any(mount.mountPoint == '/mnt/FileStore/MountFolder/' for mount in get_dbutils(spark).fs.mounts()):
AttributeError: 'FSHandler' object has no attribute 'mounts'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 11516 (child process of PID 18552) has been terminated.
SUCCESS: The process with PID 18552 (child process of PID 2604) has been terminated.
SUCCESS: The process with PID 2604 (child process of PID 15968) has been terminated.
python test.py
21/03/06 13:26:38 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:26:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:26:41 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 13:26:59 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------------------+--------------+----------------------------------------------------+------------+----------+------------------------+---------------+-------------------+------------------+---------------+---------------+---------------------------------+
|OccupancyDateTime     |Occupied_Spots|BlockfaceName                                       |SideOfStreet|Station_Id|ParkingTimeLimitCategory|Available_Spots|PaidParkingArea    |PaidParkingSubArea|PaidParkingRate|ParkingCategory|Location                         |
+----------------------+--------------+----------------------------------------------------+------------+----------+------------------------+---------------+-------------------+------------------+---------------+---------------+---------------------------------+
|08/29/2018 03:12:00 PM|6             |NE BOAT ST BETWEEN NE BOAT WR ST AND BROOKLYN AVE NE|NE          |63,125    |120                     |33             |University District|Core              |null           |Paid Parking   |POINT (-122.31616438 47.6524982) |
|08/30/2018 04:36:00 PM|4             |S WASHINGTON ST BETWEEN 3RD AVE S AND 4TH AVE S     |N           |21,773    |120                     |7              |Pioneer Square     |Edge              |null           |Paid Parking   |POINT (-122.32969194 47.60094492)|
|08/28/2018 03:57:00 PM|20            |BARNES AVE NW BETWEEN 17TH AVE NW AND NW MARKET ST  |SW          |9,141     |240                     |15             |Ballard            |Edge              |null           |Paid Parking   |POINT (-122.37968866 47.6680045) |
+----------------------+--------------+----------------------------------------------------+------------+----------+------------------------+---------------+-------------------+------------------+---------------+---------------+---------------------------------+
only showing top 3 rows


(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 172 (child process of PID 23804) has been terminated.
SUCCESS: The process with PID 23804 (child process of PID 15748) has been terminated.
SUCCESS: The process with PID 15748 (child process of PID 16836) has been terminated.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>
(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 1, in <module>
    import findspark
ModuleNotFoundError: No module named 'findspark'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 29, in <module>
    main()
  File "occupancy_etl.py", line 17, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 28, in __init__
    self._load_path = config.get('BUCKET', 'WORKING_ZONE')
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 780, in get
    d = self._unify_values(section, vars)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 1146, in _unify_values
    raise NoSectionError(section) from None
configparser.NoSectionError: No section: 'BUCKET'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 29, in <module>
    main()
  File "occupancy_etl.py", line 17, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 28, in __init__
    self._load_path = config.get('BUCKET', 'WORKING_ZONE')
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 780, in get
    d = self._unify_values(section, vars)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 1146, in _unify_values
    raise NoSectionError(section) from None
configparser.NoSectionError: No section: 'BUCKET'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>
(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 788, in get
    value = d[option]
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\collections\__init__.py", line 916, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\collections\__init__.py", line 908, in __missing__
    raise KeyError(key)
KeyError: 'processed_zone'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "occupancy_etl.py", line 29, in <module>
    main()
  File "occupancy_etl.py", line 17, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 29, in __init__
    self._save_path = config.get('BUCKET', 'PROCESSED_ZONE')
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\configparser.py", line 791, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'processed_zone' in section: 'BUCKET'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 13:48:21 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:48:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:48:24 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 15520 (child process of PID 4268) has been terminated.
SUCCESS: The process with PID 4268 (child process of PID 5552) has been terminated.
SUCCESS: The process with PID 5552 (child process of PID 4888) has been terminated.
python occupancy_etl.py
21/03/06 13:50:36 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:50:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:50:40 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 22876 (child process of PID 4084) has been terminated.
SUCCESS: The process with PID 4084 (child process of PID 15036) has been terminated.
SUCCESS: The process with PID 15036 (child process of PID 5592) has been terminated.
python occupancy_etl.py
21/03/06 13:52:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:52:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:52:23 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 7512 (child process of PID 21452) has been terminated.
SUCCESS: The process with PID 21452 (child process of PID 10776) has been terminated.
SUCCESS: The process with PID 10776 (child process of PID 13304) has been terminated.
python occupancy_etl.py
21/03/06 13:56:05 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:56:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:56:08 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 60, in transform_load_parking_hist_occupancy
    self._load_path+"2020_Paid_Parking.csv"])
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\readwriter.py", line 182, in load
    return self._df(self._jreader.load(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0: 'dbfs:/mnt/FileStore/MountFolder/'2018_Paid_Parking.csv

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 24200 (child process of PID 4616) has been terminated.
SUCCESS: The process with PID 4616 (child process of PID 19264) has been terminated.
SUCCESS: The process with PID 19264 (child process of PID 12404) has been terminated.
python occupancy_etl.py
21/03/06 13:57:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 13:57:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 13:57:03 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

21/03/06 13:57:21 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 7416 (child process of PID 15192) has been terminated.
SUCCESS: The process with PID 15192 (child process of PID 2040) has been terminated.
SUCCESS: The process with PID 2040 (child process of PID 5060) has been terminated.
python occupancy_etl.py
21/03/06 14:00:10 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 14:00:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 14:00:13 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
21/03/06 14:00:29 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+                           (0 + 1) / 1]
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 14:00:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 190, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 15416 (child process of PID 9384) has been terminated.
SUCCESS: The process with PID 9384 (child process of PID 12136) has been terminated.
SUCCESS: The process with PID 12136 (child process of PID 8856) has been terminated.
python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 171
    def transform_load_blockface_dataset(self):
                                              ^
IndentationError: unindent does not match any outer indentation level

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 14:43:01 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 14:43:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 14:43:04 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 14:43:17 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 22, in main
    "blockface.csv" : pot.transform_load_blockface_dataset,
AttributeError: 'ParkingOccupancyLoadTransform' object has no attribute 'transform_load_blockface_dataset'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 14800 (child process of PID 8932) has been terminated.
SUCCESS: The process with PID 8932 (child process of PID 19464) has been terminated.
SUCCESS: The process with PID 19464 (child process of PID 17380) has been terminated.
python occupancy_etl.py
21/03/06 14:45:25 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 14:45:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 14:45:28 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 14:45:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
21/03/06 14:45:45 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 255, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 15300 (child process of PID 23356) has been terminated.
SUCCESS: The process with PID 23356 (child process of PID 21772) has been terminated.
SUCCESS: The process with PID 21772 (child process of PID 19632) has been terminated.
python occ                                                       python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 17, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 29, in __init__
    appName("Transforming the historical parking occupancy and blockface datasets").\
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\session.py", line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\java_gateway.py", line 102, in launch_gateway
    time.sleep(0.1)
KeyboardInterrupt

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>Terminate batch job (Y/N)?

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python oc21/03/06 15:49:13 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:49:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: C:\Users\yogit\AppData\Local\Temp\tmp9qb_3z9x\connection5185251927381661521.info
        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
        at sun.nio.fs.WindowsFileSystemProvider.newByteChannel(WindowsFileSystemProvider.java:230)
        at java.nio.file.Files.newByteChannel(Files.java:361)
        at java.nio.file.Files.createFile(Files.java:632)
        at java.nio.file.TempFileHelper.create(TempFileHelper.java:138)
        at java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:161)
        at java.nio.file.Files.createTempFile(Files.java:852)
        at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:52)
        at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                                                                           java.lang.reflect.Method.invoke(Method.java:498)
python occupancy_etl.py at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 17, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 29, in __init__
    appName("Transforming the historical parking occupancy and blockface datasets").\
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\session.py", line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 384, in getOrCreate
Terminate batch job (Y/N)?     SparkContext(conf=conf or SparkConf())
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\java_gateway.py", line 102, in launch_gateway
    time.sleep(0.1)
KeyboardInterrupt


Terminate batch job (Y/N)? (dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>Terminate batch job (Y/N)?
Terminate batch job (Y/N)?

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_udf.py

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 15:49:42 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:49:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:49:46 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:49:58 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/03/06 15:50:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 255, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 13164 (child process of PID 10100) has been terminated.
SUCCESS: The process with PID 10100 (child process of PID 20968) has been terminated.
SUCCESS: The process with PID 20968 (child process of PID 3592) has been terminated.
python occupancy_etl.py
21/03/06 15:50:56 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:50:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:50:59 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:51:13 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/03/06 15:51:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 134, in transform_load_parking_hist_2012_2017_occupancy
    occ_df = occ_df_2012_2017.withColumn("OccupancyDateTime", \
UnboundLocalError: local variable 'occ_df' referenced before assignment

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py: The process with PID 23308 (ch21/03/06 15:53:12 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)minated.
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:53:16 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:53:29 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/03/06 15:53:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 135, in transform_load_parking_hist_2012_2017_occupancy
    F.to_timestamp(occ_df.OccupancyDateTime, format="mm/dd/yyyy hh:mm:ss a"))
UnboundLocalError: local variable 'occ_df' referenced before assignment

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 13076 (child process of PID 22108) has been terminated.
SUCCESS: The process with PID 22108 (child process of PID 12012) has been terminated.
SUCCESS: The process with PID 12012 (child process of PID 20068) has been terminated.
python occupancy_etl.py
21/03/06 15:53:57 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:53:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:54:01 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:54:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
21/03/06 15:54:16 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 155, in transform_load_parking_hist_2012_2017_occupancy
    station_id_lookup=occ_df_2020.select('Station_Id','Longitude','Latitude').distinct()
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 1439, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: cannot resolve '`Longitude`' given input columns: [Available_Spots, BlockfaceName, Location, OccupancyDateTime, Occupied_Spots, PaidParkingArea, PaidParkingRate, PaidParkingSubArea, ParkingCategory, ParkingTimeLimitCategory, SideOfStreet, Station_Id];;
'Project [Station_Id#273, 'Longitude, 'Latitude]
+- Relation[OccupancyDateTime#269,Occupied_Spots#270,BlockfaceName#271,SideOfStreet#272,Station_Id#273,ParkingTimeLimitCategory#274,Available_Spots#275,PaidParkingArea#276,PaidParkingSubArea#277,PaidParkingRate#278,ParkingCategory#279,Location#280] PlaceholderRelation(data_frame_read_spec {
  user_specified_source: "csv"
  user_specified_schema {
    fields {
      name: "OccupancyDateTime"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "Occupied_Spots"
      data_type {
        sql_repr: "INT"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "BlockfaceName"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "SideOfStreet"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "Station_Id"
      data_type {
        sql_repr: "INT"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "ParkingTimeLimitCategory"
      data_type {
        sql_repr: "INT"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "Available_Spots"
      data_type {
        sql_repr: "INT"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "PaidParkingArea"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "PaidParkingSubArea"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "PaidParkingRate"
      data_type {
        sql_repr: "DOUBLE"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "ParkingCategory"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
    fields {
      name: "Location"
      data_type {
        sql_repr: "STRING"
      }
      nullable: true
      metadata {
        json_repr: "{}"
      }
    }
  }
  extra_options {
    key: "header"
    nullableValue: "true"
  }
  extra_options {
    key: "path"
    nullableValue: "dbfs:/mnt/FileStore/MountFolder/2020_Paid_Parking.csv"
  }
}
,org.apache.spark.sql.SQLContext@222f143a,StructType(StructField(OccupancyDateTime,StringType,true), StructField(Occupied_Spots,IntegerType,true), StructField(BlockfaceName,StringType,true), StructField(SideOfStreet,StringType,true), StructField(Station_Id,IntegerType,true), StructField(ParkingTimeLimitCategory,IntegerType,true), StructField(Available_Spots,IntegerType,true), StructField(PaidParkingArea,StringType,true), StructField(PaidParkingSubArea,StringType,true), StructField(PaidParkingRate,DoubleType,true), StructField(ParkingCategory,StringType,true), StructField(Location,StringType,true)),None)


(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 24380 (child process of PID 20796) has been terminated.
SUCCESS: The process with PID 20796 (child process of PID 684) has been terminated.
SUCCESS: The process with PID 684 (child process of PID 11084) has been terminated.
python occupancy_etl.py
21/03/06 15:56:26 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:56:29 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:56:43 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
21/03/06 15:56:43 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 31, in <module>
    main()
  File "occupancy_etl.py", line 27, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 162, in transform_load_parking_hist_2012_2017_occupancy
    occ_df_2020 = occ_df_2020.withColumn("Latitude",occ_df["Latitude"].cast(DoubleType())) \
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 1398, in __getitem__
    jc = self._jdf.apply(item)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: Cannot resolve column name "Latitude" among (OccupancyDateTime, Occupied_Spots, Station_Id, Available_Spots);

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 13844 (child process of PID 23512) has been terminated.
SUCCESS: The process with PID 23512 (child process of PID 8640) has been terminated.
SUCCESS: The process with PID 8640 (child process of PID 20748) has been terminated.
python occupancy_etl.py
21/03/06 15:58:04 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 15:58:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 15:58:08 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 15:58:22 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/03/06 15:58:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |0.0      |0         |0       |null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+----------+--------------+---------------+---------+--------+
|OccupancyDateTime  |Station_Id|Occupied_Spots|Available_Spots|Longitude|Latitude|
+-------------------+----------+--------------+---------------+---------+--------+
|2012-01-10 09:42:00|58797     |2             |6              |null     |null    |
|2012-01-10 10:52:00|58797     |4             |6              |null     |null    |
|2012-01-07 15:20:00|58797     |2             |6              |null     |null    |
+-------------------+----------+--------------+---------------+---------+--------+
only showing top 3 rows


(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 23784 (child process of PID 2620) has been terminated.
SUCCESS: The process with PID 2620 (child process of PID 5516) has been terminated.
SUCCESS: The process with PID 5516 (child process of PID 13892) has been terminated.
occ_df_2012_2017.show(10,truncate=False)
'occ_df_2012_2017.show' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>occ_df_2012_2017.show(10,truncate=False)
'occ_df_2012_2017.show' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 16, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 29, in __init__
    appName("Transforming the historical parking occupancy and blockface datasets").\
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\session.py", line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\java_gateway.py", line 102, in launch_gateway
    time.sleep(0.1)
KeyboardInterrupt

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>Terminate batch job (Y/N)?
Terminate batch job (Y/N)?
Y
'Y' is not recognized as an internal or external command,
operable program or batch file.

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 18:38:47 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:38:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:38:50 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:39:03 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 18:39:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 275, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 11568 (child process of PID 18320) has been terminated.
SUCCESS: The process with PID 18320 (child process of PID 17244) has been terminated.
SUCCESS: The process with PID 17244 (child process of PID 3128) has been terminated.
python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 19, in <module>
    class ParkingOccupancyLoadTransform:
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 47, in ParkingOccupancyLoadTransform
    udf_format_minstoHHMMSS=udf(lambda x: format_minstoHHMMSS(x))
NameError: name 'udf' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 18:42:39 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:42:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:42:42 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:42:54 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 281, in transform_load_blockface_dataset
    blockface=blockface.withColumn('wkd_start1',udf_format_minstoHHMMSS('wkd_start1')) \
NameError: name 'udf_format_minstoHHMMSS' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 13052 (child process of PID 3432) has been terminated.
SUCCESS: The process with PID 3432 (child process of PID 16628) has been terminated.
SUCCESS: The process with PID 16628 (child process of PID 15384) has been terminated.
python occupancy_etl.py
21/03/06 18:44:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:44:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:44:23 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:44:36 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 281, in transform_load_blockface_dataset
    blockface=blockface.withColumn('wkd_start1',udf_format_minstoHHMMSS('wkd_start1')) \
NameError: name 'udf_format_minstoHHMMSS' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 9388 (child process of PID 14732) has been terminated.
SUCCESS: The process with PID 14732 (child process of PID 20940) has been terminated.
SUCCESS: The process with PID 20940 (child process of PID 15556) has been terminated.
python occupancy_etl.py
21/03/06 18:54:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:54:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:54:23 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:54:38 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+                           (0 + 1) / 1]
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 18:54:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 276, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 24484 (child process of PID 11120) has been terminated.
SUCCESS: The process with PID 11120 (child process of PID 23952) has been terminated.
SUCCESS: The process with PID 23952 (child process of PID 11072) has been terminated.
python occupancy_etl.py
21/03/06 18:56:49 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:56:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:56:52 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:57:05 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 263, in transform_load_blockface_dataset
    blockface=blockface.withColumn('wkd_start1',occupancy_udf.udf_format_minstoHHMMSS('wkd_start1')) \
NameError: name 'occupancy_udf' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 20804 (child process of PID 6692) has been terminated.
SUCCESS: The process with PID 6692 (child process of PID 22352) has been terminated.
SUCCESS: The process with PID 22352 (child process of PID 2724) has been terminated.
python occupancy_etl.py
21/03/06 18:57:40 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 18:57:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 18:57:43 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 18:57:54 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 18:58:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 276, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 4184 (child process of PID 21324) has been terminated.
SUCCESS: The process with PID 21324 (child process of PID 3600) has been terminated.
SUCCESS: The process with PID 3600 (child process of PID 16336) has been terminated.
python                                                           python occupancy_udf.py

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 19:00:53 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 19:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 19:00:56 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
21/03/06 19:01:14 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 19:01:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 276, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 7648 (child process of PID 14340) has been terminated.
SUCCESS: The process with PID 14340 (child process of PID 3420) has been terminated.
SUCCESS: The process with PID 3420 (child process of PID 2672) has been terminated.
python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 16, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 32, in __init__
    appName("Transforming the historical parking occupancy and blockface datasets").\
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\session.py", line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\context.py", line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\java_gateway.py", line 102, in launch_gateway
Terminate batch job (Y/N)?     time.sleep(0.1)
KeyboardInterrupt

Terminate batch job (Y/N)?

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_udf.py

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 19:02:20 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 19:02:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 19:02:23 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 19:02:35 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 19:02:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
Traceback (most recent call last):                                  (0 + 1) / 1]
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 26, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 276, in transform_load_blockface_dataset
    blockface.show(3, truncate=False)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\dataframe.py", line 443, in show
    print(self._jdf.showString(n, int(truncate), vertical))
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\py4j\java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\yogit\anaconda3\envs\dbconnect\lib\site-packages\pyspark\sql\utils.py", line 133, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf''. Full traceback below:
Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/worker.py", line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/databricks/spark/python/pyspark/worker.py", line 464, in read_udfs
    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))
  File "/databricks/spark/python/pyspark/worker.py", line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/databricks/spark/python/pyspark/worker.py", line 75, in read_command
    command = serializer._read_with_length(file)
  File "/databricks/spark/python/pyspark/serializers.py", line 180, in _read_with_length
    raise SerializationError("Caused by " + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File "/databricks/spark/python/pyspark/serializers.py", line 177, in _read_with_length
    return self.loads(obj)
  File "/databricks/spark/python/pyspark/serializers.py", line 466, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'occupancy_udf'



(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 8084 (child process of PID 9000) has been terminated.
SUCCESS: The process with PID 9000 (child process of PID 9056) has been terminated.
SUCCESS: The process with PID 9056 (child process of PID 12432) has been terminated.
python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 286
    .withColumn('sat_start2'udf_format_minstoHHMMSS('sat_start2')) \
                                                  ^
SyntaxError: invalid syntax

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 289
    .withColumn('sat_end3',.udf_format_minstoHHMMSS('sat_end3'))
                           ^
SyntaxError: invalid syntax

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 13, in <module>
    import occupancy_udf
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_udf.py", line 22, in <module>
    spark.udf.register("udf_format_minstoHHMMSS", format_minstoHHMMSS, StringType())
NameError: name 'spark' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 13, in <module>
    import occupancy_udf
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_udf.py", line 22, in <module>
    spark.udf.register("udf_format_minstoHHMMSS", format_minstoHHMMSS, StringType())
NameError: name 'spark' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 22, in <module>
    class ParkingOccupancyLoadTransform:
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 49, in ParkingOccupancyLoadTransform
    self.spark.udf.register("udf_format_minstoHHMMSS", format_minstoHHMMSS, StringType())
NameError: name 'self' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 19:11:06 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 19:11:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 19:11:08 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
Traceback (most recent call last):
  File "occupancy_etl.py", line 30, in <module>
    main()
  File "occupancy_etl.py", line 16, in main
    pot = ParkingOccupancyLoadTransform()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 34, in __init__
    self.spark.udf.register("udf_format_minstoHHMMSS", format_minstoHHMMSS, StringType())
NameError: name 'format_minstoHHMMSS' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 23616 (child process of PID 19404) has been terminated.
SUCCESS: The process with PID 19404 (child process of PID 17600) has been terminated.
SUCCESS: The process with PID 17600 (child process of PID 3492) has been terminated.
python occupancy_etl.py
Traceback (most recent call last):
  File "occupancy_etl.py", line 8, in <module>
    from occupancy_transform import ParkingOccupancyLoadTransform
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 13, in <module>
    import occupancy_udf
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_udf.py", line 6, in <module>
    @udf(StringType())
NameError: name 'StringType' is not defined

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>python occupancy_etl.py
21/03/06 19:25:54 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 19:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 19:25:58 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 19:26:09 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
Traceback (most recent call last):
  File "occupancy_etl.py", line 43, in <module>
    main()
  File "occupancy_etl.py", line 39, in main
    modules[file]()
  File "C:\Step 6 - Scale Your Prototype\2 - data processing\occupancy_transform.py", line 54, in transform_load_parking_hist_2018_2020_occupancy
    occ_df_2018_2020 = self.spark.read.format("csv") \
AttributeError: 'ParkingOccupancyLoadTransform' object has no attribute 'spark'

(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 11608 (child process of PID 17784) has been terminated.
SUCCESS: The process with PID 17784 (child process of PID 20708) has been terminated.
SUCCESS: The process with PID 20708 (child process of PID 2072) has been terminated.
python occupancy_etl.py
21/03/06 19:26:47 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2680)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2680)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/03/06 19:26:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/06 19:26:50 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/03/06 19:27:02 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+--------------+----------+---------------+-------------+-----------+
|OccupancyDateTime  |Occupied_Spots|Station_Id|Available_Spots|Longitude    |Latitude   |
+-------------------+--------------+----------+---------------+-------------+-----------+
|2018-01-29 15:12:00|6             |63125     |33             |-122.31616438|47.6524982 |
|2018-01-30 16:36:00|4             |21773     |7              |-122.32969194|47.60094492|
|2018-01-28 15:57:00|20            |9141      |15             |-122.37968866|47.6680045 |
+-------------------+--------------+----------+---------------+-------------+-----------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+-----------+-------+
|OccupancyDateTime  |day_of_week|month  |
+-------------------+-----------+-------+
|2018-01-29 15:12:00|Monday     |January|
|2018-01-30 16:36:00|Tuesday    |January|
|2018-01-28 15:57:00|Sunday     |January|
+-------------------+-----------+-------+
only showing top 3 rows

21/03/06 19:27:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|station_id|station_address                                                            |side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
|124518    |I5 NB SR520 EB ON RP BETWEEN I5 NB AND SR520 EB                            |SE  |2200     |None            |0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |0         |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|null              |null   |
|122195    |WEST SEATTLE BRIDGE TRL BETWEEN SW SPOKANE NR ST AND WEST SEATTLE BR NR TRL|S   |1000     |None            |0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |0         |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|null              |null   |
|124562    |NE 50TH ST OFF RP BETWEEN I5 NB AND 7TH AVE NE                             |W   |4100     |None            |0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |0         |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|0.0      |00:00:00  |00:00:00|null              |null   |
+----------+---------------------------------------------------------------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+
only showing top 3 rows

View job details at https://adb-1387481425025837.17.azuredatabricks.net/?o=1387481425025837#/setting/clusters/0306-174454-gears22/sparkUi
+-------------------+----------+--------------+---------------+-------------+-----------+
|OccupancyDateTime  |Station_Id|Occupied_Spots|Available_Spots|Longitude    |Latitude   |
+-------------------+----------+--------------+---------------+-------------+-----------+
|2012-01-07 11:42:00|58197     |1             |10             |-122.32416226|47.61080097|
|2012-01-09 12:31:00|58197     |1             |10             |-122.32416226|47.61080097|
|2012-01-09 13:07:00|58197     |2             |10             |-122.32416226|47.61080097|
+-------------------+----------+--------------+---------------+-------------+-----------+
only showing top 3 rows


(dbconnect) C:\Step 6 - Scale Your Prototype\2 - data processing>SUCCESS: The process with PID 18616 (child process of PID 7208) has been terminated.
SUCCESS: The process with PID 7208 (child process of PID 18908) has been terminated.
SUCCESS: The process with PID 18908 (child process of PID 3148) has been terminated.
